{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/internal/generation_utils#generate-outputs\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\n",
    "# Greedy sampling\n",
    "output_greedy = model.generate(\n",
    "    **inputs, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True, \n",
    "    max_new_tokens=128,\n",
    "    num_return_sequences=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, my dog is cute and icky. I'm not sure if she's a good dog, but she's cute and icky. I'm not sure if she's a good dog, but she's cute and icky.\\n\\nI'm not sure if she's a good dog, but she's cute and icky. I'm not sure if she's a good dog, but she's cute and icky.\\n\\nI'm not sure if she's a good dog, but she's cute and icky. I'm not sure if she's a good dog, but she's cute and icky.\\n\\nI'm not sure if she\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output_greedy.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"icky .  I 'm  not  sure  if  she 's  a  good  dog ,  but  she 's  cute  and   icky .  I 'm  not  sure  if  she 's  a  good  dog ,  but  she 's  cute  and   icky . \\n \\n I 'm  not  sure  if  she 's  a  good  dog ,  but  she 's  cute  and   icky .  I 'm  not  sure  if  she 's  a  good  dog ,  but  she 's  cute  and   icky . \\n \\n I 'm  not  sure  if  she 's  a  good  dog ,  but  she 's  cute  and   icky .  I 'm  not  sure  if  she 's  a  good  dog ,  but  she 's  cute  and   icky . \\n \\n I 'm  not  sure  if  she\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual greedy sampling\n",
    "sm = torch.nn.functional.softmax(torch.stack(output_greedy['scores']), dim=-1)\n",
    "topk = sm.topk(k=1, dim=-1)\n",
    "\" \".join(tokenizer.batch_decode(topk.indices.reshape(-1), skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other sampling methods\n",
    "\n",
    "https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "https://huggingface.co/blog/introducing-csearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hello, my dog is cute and icky, but I'm not sure if he's a good dog or not. I'm not sure if he's a good dog or not. I'm not sure if he's a good dog or not.\"]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Beam search\n",
    "beam_output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "tokenizer.batch_decode(beam_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hello, my dog is cute and icky, but I'm not sure if he's a good dog or not.\\n\\nI'm a big fan of dogs, and I love them. But I don't think I've ever seen a dog\"]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Beam search with n-gram penalty\n",
    "beam_output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2,  # n-gram penalty\n",
    "    early_stopping=True\n",
    ")\n",
    "tokenizer.batch_decode(beam_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello, my dog is cute and \\xa0fancy. \\xa0I was thinking at the time the bunny was quite far and moved out of the equation but I hear she so far has been very affectionate and supportive with everyone around her and her']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sampling\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.manual_seed(0.)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0\n",
    ")\n",
    "tokenizer.batch_decode(sample_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello, my dog is cute and icky and she\\'s always been my new pet.\"\\n\\nHe added that it helped him that he was comfortable with his owner\\'s love of the dog and the positive attention that went with his actions.']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Top-K sampling\n",
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50\n",
    ")\n",
    "tokenizer.batch_decode(sample_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello, my dog is cute and iced and tastes like a treat! Today I gave my 2 year old veggie litters 3-4 steamed treats, and treat on it, and the kids love it, and the father loves it!']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Nucleous sampling\n",
    "nucleous_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.95, \n",
    "    top_k=0\n",
    ")\n",
    "tokenizer.batch_decode(nucleous_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hello, my dog is cute and ive always liked...I wanted the weight off. Unfortunately, everything failed at my pooch car wash.Also your child's final look.\n",
      "\n",
      "\n",
      "User Info: Xxxxy Xxxxy 2 years ago\n",
      "\n",
      "1: Hello, my dog is cute and ive done everything i have ever wanted but I haven't seen a dog with flesh. I can't even imagine what it's like to be apart. He's some fucking torokoo dog to shit on while\n",
      "\n",
      "2: Hello, my dog is cute and icky. She is 9 years old, and still gives me great pleasure and affection. However, some point in her adolescence slowly sets in (she does not love me!). This lead me to one of the last\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Nucleous sampling\n",
    "nucleous_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.95, \n",
    "    top_k=0,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "for i, sample_output in enumerate(nucleous_output):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hello, my dog is cute and icky. I'm going to try to get her to eat a bowl of cereal.\\n\\nI'm going to try to get her to eat a bowl of cereal. I'm going to try to get her to eat a bowl of cereal. I'm going to try to get her to eat a bowl of cereal. I'm going to try to get her to eat a bowl of cereal. I'm going to try to get her to eat a bowl of cereal. I'm going to try to get her to eat a bowl of cereal. I'm going to try to get her to eat a\"]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Contrastive search\n",
    "\n",
    "contrastive_output = model.generate(\n",
    "    **inputs, \n",
    "    penalty_alpha=0.6, \n",
    "    top_k=4, \n",
    "    max_length=128\n",
    ")\n",
    "tokenizer.batch_decode(contrastive_output, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_safety",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 | packaged by conda-forge | (default, Sep 13 2021, 21:46:58) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "237a4bf569148332ef815a7b4e6f7648dccdaf57e09168fd0e3e56117530a0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
